import torch
import torch.nn as nn

class CrossAttentionTextImage(nn.Module):
    def __init__(self, dim_text, dim_image, hidden_dim):
        super().__init__()
        
        # 임베딩 차원을 동일한 크기로 변환
        self.text_proj = nn.Linear(dim_text, hidden_dim)
        self.image_proj = nn.Linear(dim_image, hidden_dim)

        # Cross Attention: Query=text, Key/Value=image
        self.cross_attn = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=4, batch_first=True)

        # Optional: 후처리
        self.output_proj = nn.Linear(hidden_dim, hidden_dim)

    def forward(self, text_emb, image_emb):
        # (B, D) → (B, 1, H)
        query = self.text_proj(text_emb).unsqueeze(1)   # (B, 1, H)
        key_value = self.image_proj(image_emb).unsqueeze(1)  # (B, 1, H)

        # Cross Attention
        attn_output, _ = self.cross_attn(query, key_value, key_value)  # (B, 1, H)
        fused = attn_output.squeeze(1)  # (B, H)

        return self.output_proj(fused)  # (B, H)
